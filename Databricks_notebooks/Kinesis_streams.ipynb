{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be3f165c-6a81-472e-bcb1-181e936308c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8772279-7efb-4692-9b1e-6ae062de627a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructField, StringType, StructType, TimestampType, IntegerType\n",
    "import urllib\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7edf022-025c-4246-943f-ada6e7020f9c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Reading the Kinesis streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61dc4a33-3e2e-4046-8f95-b8ff91934da0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ba27655-bcb2-4ac0-b2f6-0f55cdeec94a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Reading the credential file and extractin the keys needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14a8b16a-0401-4637-bc37-4cffa8514a6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secret key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f48cb6bf-50d2-42fa-a740-1c9959922da3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create a function that reads the  kinesis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d42f821-7c66-4fc7-b705-c0e0feca4658",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_kinesis_data(stream_name, json_schema):\n",
    "\n",
    "    ''' The function utilizes `spark` to read the Kinesis stream. It proceeds by deserializing the data, which involves:\n",
    "- Retrieving the 'data' field from the Kinesis stream and converting it into a string.\n",
    "- Parsing the 'data' field based on the provided JSON schema.\n",
    "- Selecting all columns from the parsed data.\n",
    "\n",
    "Args:\n",
    "    stream_name (`string`): The name of the stream from which data is to be streamed.\n",
    "    json_schema (`StructType`): The schema to be applied to the output dataframe.\n",
    "\n",
    "Returns:\n",
    "    df (`pyspark.sql.DataFrame`): A DataFrame with the specified schema and alias.\n",
    "'''\n",
    "\n",
    " # Creating the kinesis_stream dataframe\n",
    "    kinesis_stream = spark \\\n",
    "        .readStream \\\n",
    "        .format('kinesis') \\\n",
    "        .option('streamName', stream_name) \\\n",
    "        .option('initialPosition', 'earliest') \\\n",
    "        .option('region', 'us-east-1') \\\n",
    "        .option('awsAccessKey', ACCESS_KEY) \\\n",
    "        .option('awsSecretKey', SECRET_KEY) \\\n",
    "        .load()\n",
    "\n",
    "    # deserialising the data\n",
    "    df = kinesis_stream \\\n",
    "    .selectExpr(\"CAST(data as STRING)\") \\ \n",
    "    .withColumn(\"data\", from_json(col(\"data\"), json_schema)) \\ \n",
    "    .select(col(\"data.*\")) \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbb10bee-103d-4c44-a492-c2f35001fc41",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Writing to Delta Tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb4f252d-636d-4e67-a70d-590647e14681",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A function is created to write transformed data to delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3cca6c3-372e-49cb-bdbe-6d6c95a6f281",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_kinesis_data(table_name, df):\n",
    "    ''' A function that writes data to the specified delta table, it also \n",
    "    checkpoints the data incase we need to roll it back to an older state.\n",
    "\n",
    "    Args:\n",
    "        table_name (`string`): The desired name for the delta table\n",
    "        df (`pyspark.sql.DataFrame`): \n",
    "    \n",
    "    '''\n",
    "\n",
    "    df.writeStream \\ \n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\ \n",
    "    .option(\"checkpointLocation\", f\"/tmp/kinesis/{table_name}_checkpoints/\") \\ \n",
    "    .table(table_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7aaf6c8-806a-4011-b9f5-7940422c3bdd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Cleaning\n",
    "Read the pin streaming data,clean and upload to appropriate delta table\n",
    "- stream_name = '<name_of_kinesis_stream>'\n",
    "- table_name = '<desired_table_name>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8ec41e7-cb91-45c5-8ec8-27459b5c0bcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# defining the stream name\n",
    "stream_name =''\n",
    "\n",
    "json_schema = StructType([\n",
    "    StructField('index', IntegerType()),\n",
    "    StructField('unique_id', StringType()),\n",
    "    StructField('title', StringType()),\n",
    "    StructField('description', StringType()),\n",
    "    StructField('follower_count', StringType()),\n",
    "    StructField('poster_name', StringType()),\n",
    "    StructField('tag_list', StringType()),\n",
    "    StructField('is_image_or_video', StringType()),\n",
    "    StructField('image_src', StringType()),\n",
    "    StructField('save_location', StringType()),\n",
    "    StructField('category', StringType())\n",
    "])\n",
    "\n",
    "    \n",
    "    # Creating the kinesis_stream dataframe\n",
    "    df_pin = spark \\\n",
    "        .readStream \\\n",
    "        .format('kinesis') \\\n",
    "        .option('streamName', stream_name) \\\n",
    "        .option('initialPosition', 'earliest') \\\n",
    "        .option('region', 'us-east-1') \\\n",
    "        .option('awsAccessKey', ACCESS_KEY) \\\n",
    "        .option('awsSecretKey', SECRET_KEY) \\\n",
    "        .load()\n",
    "\n",
    "    # deserialising the data\n",
    "    df_pin = df_pin \\\n",
    "        .selectExpr(\"CAST(data as STRING)\") \\\n",
    "        .withColumn(\"data\", from_json(col(\"data\"), json_schema)) \\\n",
    "        .select(col(\"data.*\")) \n",
    "\n",
    "    return df_pin\n",
    "\n",
    "df_pin = read_kinesis_data(stream_name,json_schema)\n",
    "\n",
    "# replacing invalid entries with `None`, it is best to define a dictionary \n",
    "# here as this makes the whole process more scalable. \n",
    "\n",
    "col_and_entries_to_replace = {\n",
    "    'title': 'No Title Data Available',\n",
    "    'description': 'No description available Story format',\n",
    "    'follower_count': 'User Info Error',\n",
    "    'poster_name': 'User Info Error',\n",
    "    'image_src': 'Image src error.',\n",
    "    'tag_list': 'N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e'\n",
    "}\n",
    "\n",
    "\n",
    "# Iterate over the dictionary and perform replacements\n",
    "for column, value in col_and_entries_to_replace.items():\n",
    "    df_pin = df_pin.withColumn(column, when(df_pin[column] == value, None).otherwise(df_pin[column]))\n",
    "\n",
    "# replacing 'k' and 'M' with '000' and '000000' respectively.\n",
    "df_pin = df_pin.withColumn('follower_count', \n",
    "                                           when(df_pin_cleaned.follower_count.endswith('k'), regexp_replace(df_pin_cleaned.follower_count, 'k', '000'))\n",
    "                                           .when(df_pin_cleaned.follower_count.endswith('M'), regexp_replace(df_pin_cleaned.follower_count, 'M', '000000'))\n",
    "                                           .otherwise(df_pin_cleaned.follower_count))\n",
    "\n",
    "# casting follower count to integers\n",
    "df_pin = df_pin.withColumn('follower_count', df_pin.follower_count.cast('int'))\n",
    "\n",
    "# making the 'save_location' column show the path\n",
    "df_pin = df_pin.withColumn('save_location', regexp_replace(df_pin.save_location, 'Local save in ', ''))\n",
    "\n",
    "# defining the delta table name.\n",
    "table_name = ''\n",
    "\n",
    "# writing the data to the delta tables. \n",
    "write_kinesis_data(table_name, df_pin)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dc49dce-8f96-44b6-bca5-e5f48cb4e0c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Read the geo streaming data,clean and upload to appropriate delta table\n",
    "- stream_name = '<name_of_kinesis_stream>'\n",
    "- table_name = '<desired_table_name>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11ee0cea-25e9-41c1-8aa8-798c895e9186",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "json_schema = StructType([\n",
    "    StructField('ind', IntegerType()),\n",
    "    StructField('country', StringType()),\n",
    "    StructField('latitude', StringType()),\n",
    "    StructField('longitude', StringType()),\n",
    "    StructField('timestamp', TimestampType())])\n",
    "\n",
    "    \n",
    "    # Creating the kinesis_stream dataframe\n",
    "    df_geo = spark \\\n",
    "        .readStream \\\n",
    "        .format('kinesis') \\\n",
    "        .option('streamName', stream_name) \\\n",
    "        .option('initialPosition', 'earliest') \\\n",
    "        .option('region', 'us-east-1') \\\n",
    "        .option('awsAccessKey', ACCESS_KEY) \\\n",
    "        .option('awsSecretKey', SECRET_KEY) \\\n",
    "        .load()\n",
    "\n",
    "    # deserialising the data\n",
    "    df_geo = df_geo \\\n",
    "        .selectExpr(\"CAST(data as STRING)\") \\\n",
    "        .withColumn(\"data\", from_json(col(\"data\"), json_schema)) \\\n",
    "        .select(col(\"data.*\")) \n",
    "\n",
    "    return df_geo\n",
    "\n",
    "df_geo = read_kinesis_data(stream_name.json_schema)\n",
    "\n",
    "\n",
    "# creating a new column called 'coordinates'\n",
    "df_geo = df_geo.withColumn('coordinates', array(df_geo.latitude, df_geo.longitude))\n",
    "\n",
    "# dropping the columns 'latitude' and 'longitude'\n",
    "df_geo = df_geo.drop(*['latitude', 'longitude'])\n",
    "\n",
    "# converting 'timestamp' to a timestamp data type.\n",
    "df_geo = df_geo.withColumn('timestamp', to_timestamp(df_geo.timestamp))\n",
    "\n",
    "# defining the delta table name.\n",
    "table_name = ''\n",
    "\n",
    "# writing the data to the delta tables. \n",
    "write_kinesis_data(table_name, df_geo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59b0ca7b-9edb-4806-8db1-4beb82169833",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Read the user streaming data,clean and upload to appropriate delta table\n",
    "- stream_name = '<name_of_kinesis_stream>'\n",
    "- table_name = '<desired_table_name>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2409b322-4f22-4a3c-89fc-07cb9b5bbe93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "json_schema = StructType([\n",
    "    StructField('ind', IntegerType()),\n",
    "    StructField('first_name', StringType()),\n",
    "    StructField('last_name', StringType()),\n",
    "    StructField('age', IntegerType()),\n",
    "    StructField('date_joined', TimestampType())])\n",
    "\n",
    "    \n",
    "    # Creating the kinesis_stream dataframe\n",
    "    df_user = spark \\\n",
    "        .readStream \\\n",
    "        .format('kinesis') \\\n",
    "        .option('streamName', stream_name) \\\n",
    "        .option('initialPosition', 'earliest') \\\n",
    "        .option('region', 'us-east-1') \\\n",
    "        .option('awsAccessKey', ACCESS_KEY) \\\n",
    "        .option('awsSecretKey', SECRET_KEY) \\\n",
    "        .load()\n",
    "\n",
    "    # deserialising the data\n",
    "    df_user = df_user \\\n",
    "        .selectExpr(\"CAST(data as STRING)\") \\\n",
    "        .withColumn(\"data\", from_json(col(\"data\"), json_schema)) \\\n",
    "        .select(col(\"data.*\")) \n",
    "\n",
    "    return df_user\n",
    "\n",
    "df_user = read_kinesis_data(stream_name)\n",
    "\n",
    "# creating a new column called 'user_name'\n",
    "df_user = df_user.withColumn('user_name', concat(df_user.first_name, lit(' '), df_user.last_name))\n",
    "\n",
    "# dropping the 'first_name' and 'last_name' columns\n",
    "df_user = df_user.drop(*['first_name', 'last_name'])\n",
    "\n",
    "# converting 'date_joined' to a timestamp data type.\n",
    "df_user = df_user.withColumn('date_joined', to_timestamp(df_user_cleaned.date_joined))\n",
    "\n",
    "# reordering columns\n",
    "df_user = df_user.select('ind', 'user_name', 'age', 'date_joined')\n",
    "\n",
    "# defining the delta table name.\n",
    "table_name = ''\n",
    "\n",
    "# writing the data to the delta tables. \n",
    "write_kinesis_data(table_name, df_user)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 852830702702674,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Kinesis_streams",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
